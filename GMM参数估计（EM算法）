## EM算法与高斯混合模型（GMM）
EM算法是一种迭代算法，1977年由Dempster等人总结提出，用于含有隐变量（hidden variable）的概率模型参数的极大似然估计，或极大后验概率估计。EM算法的每次迭代由2步组成：

1. E-step：求期望（expectation）
2. M-step：最大化（maximization）

所以该算法称为期望极大算法（expectation maximization algorithm），简称EM算法。EM算法的一个重要应用是高斯混合模型（GMM）的参数估计。这里以学习笔记的方式，梳理EM算法在GMM参数估计中的主要过程，记录如下：

- **【k-mixture GMM的概率分布】**
$$ P(X | \Theta) =  \sum_{l=1}^k \alpha_l N(X | \theta_l) = \sum_{l=1}^k \alpha_l N(X | \mu_l, \Sigma_l) $$
其中，$\Theta = \{ \alpha_1,...,\alpha_k;\theta_1,...,\theta_k\} = \{ \alpha_1,...,\alpha_k;\mu_1,...,\mu_k;\Sigma_1,...,\Sigma_k \}$，$\alpha_l$代表各高斯的权重，$\sum_{l=1}^k \alpha_l = 1$。因此，由$k$个高斯分布分模型组成的GMM可以理解为：这$k$个高斯分布以不同的权重，组合成一个混合模型，混合模型产生的每一个样本点$x_i$都受到$k$个高斯分布的不同影响，影响的大小由权重$\alpha_l$决定。举个不太恰当的例子帮助理解：
$$A、B国的混血女 + C、D国的混血男 = 子女_1 + ... + 子女_n$$
其中，$A、B、C、D$代表4个不同的高斯分布，它们以一定的权重形成一个高斯混合模型，这个模型生出来的每一个$子女_i$，都会受到$A、B、C、D$四国血统的影响，影响的大小由$A、B、C、D$结合时所给出基因多少（权重）决定。

- **【k-mixture GMM的对数似然函数】**
$$ L(\Theta | X) = logP(X | \Theta) = log[\sum_{l=1}^k \alpha_l N(X | \mu_l, \Sigma_l)] = log[\prod_{i=1}^n \sum_{l=1}^k \alpha_l N(x_i | \mu_l, \Sigma_l)] =  \sum_{I=1}^n [ log(\sum_{l=1}^k \alpha_l N(x_i | \mu_l, \Sigma_l))] $$
我们的目标是求出使$L(\Theta | X)$最大的各个高斯参数，即$\Theta_{MLE} = argmax\{logP(X | \Theta)\}$。但是：$log$中带有$\sum$符号不利于求导；求导后令其为零也很难得到参数集$\Theta$中的众多参数。

- **【EM算法的基本思想】**
当无法一步到位地将$\Theta_{MLE}$求出来时，可以通过迭代的方式，从初始参数$\Theta^{(1)}$$\to$$\Theta^{(2)}$$\to$...$\to$$\Theta^{(f)}$，直到下一次迭代更新，参数的变化十分小，即可认为其收敛，并得到最优参数$\Theta^{(f)}$。
既然是要使对数似然函数最大化，那么每一步迭代都必须保证：$$log[P(X | \Theta^{(g+1)})] \ge log[X | P(\Theta^{(g)})] $$
并且，EM算法给出的关于$\Theta^{(g+1)}$和$\Theta^{(g)}$之间满足的关系是：$$\Theta^{(g+1)} = argmax(\int_z P(Z | X,\Theta^{(g)})logP(X,Z | \Theta)dz) $$ 
其中，$\Theta^{(g)}$是上一次迭代得到的参数，$\Theta^{(g+1)}$是下一次迭代更新的参数。$Z$称为隐变量，它是一种不可观测的辅助变量。隐变量的添加必须满足：
1. 加入隐变量后能够简化模型的解法
2. 在概率模型中，加入隐变量后不能改变数据的边缘分布，即要满足$P(X | \Theta) = \int_z P(X | Z, \Theta) P(Z | \Theta)dz$
 
- **【GMM中的隐变量$ Z = \{z_1,z_2,...,z_n\}$】**
观测数据$x_i$（$i=1,2,...,n$）是这样产生的：首先依照各高斯的权重$\alpha_l$，选出第$l$个高斯分布，然后依照第$l$个高斯的概率分布$N(X | \theta_l)$生成观测数据$x_i$。这时观测数据$x_i$（$i=1,2,...,n$）是已知的，反映观测数据$x_i$属于哪一个高斯分模型的数据是未知的，将这个未知的、观测不到的数据称为隐变量$z_i$（$i=1,2,...,n$）。显然，$z_i\in\{1,2,...,k\}$。$$x_i \to z_i：样本点x_i属于第z_i个高斯分布$$

- **【EM算法的收敛性】**
EM算法的核心就是按照$\Theta^{(g+1)}$和$\Theta^{(g)}$之间的等式关系，不断去更新参数，并且能保证每一次更新，都使得对数似然函数逐渐增大。证明EM算法的收敛性：$$P(X | \Theta) = \frac{P(X,Z | \Theta)}{P(Z | X, \Theta)}$$
等式两边取对数并以$P(Z | X, \Theta^{(g)})$为概率分布求期望：$$E[logP(X | \Theta)] = E[logP(X,Z | \Theta)] - E[logP(Z |X, \Theta)]$$
则等式左边写为：$$ \int_z P(Z | X, \Theta^{(g)})logP(X | \Theta)dz = logP(X | \Theta) \int_z P(Z | X, \Theta^{(g)})dz = logP(X | \Theta)$$
等式右边写为：$$ \int_z P(Z | X, \Theta^{(g)})logP(X,Z | \Theta)dz - \int_z P(Z | X, \Theta^{(g)})logP(Z |X, \Theta)dz = Q(\Theta, \Theta^{(g)}) - H(\Theta, \Theta^{(g)})$$
由Jensens不等式可以证明$H(\Theta^{(g)},\Theta^{(g)}) \ge H(\Theta,\Theta^{(g)})，\forall \Theta$
从而对于任意一次迭代更新参数$\Theta^{(g+1)}$，都有$H(\Theta^{(g)},\Theta^{(g)}) \ge H(\Theta^{(g+1)},\Theta^{(g)})$
因此，只要$argmax\{Q(\Theta, \Theta^{(g)})\}$，就能保证对数似然函数的最大化。注意$Q$函数中，$\Theta^{(g)}$是上一次迭代后得到的固定常数，而$\Theta$是一个变量，作$argmax$不会改变$\Theta^{(g)}$的值。

- **【EM算法的核心定义：E-step】**
上述$Q(\Theta, \Theta^{(g)})$可以看作函数$logP(X,Z | \Theta)$以概率分布$P(Z | X, \Theta^{(g)})$求期望，其定义如下：
1. 观测值与隐变量的联合概率：$$P(X,Z | \Theta) = \prod_{i=1}^n p(x_i,z_i | \Theta) = \prod_{i=1}^n p(x_i | z_i,\Theta) p(z_i | \Theta) = \prod_{i=1}^n \alpha_{z_i}N(x_i | \mu_{z_i},\Sigma_{z_i})$$
2. 在对应观测值已知的情况下，该观测值来源于哪个高斯的条件概率：
$$P(Z | X, \Theta^{(g)}) = \prod_{i=1}^n p(z_i | x_i, \Theta^{(g)}) = \prod_{i=1}^n \frac{\alpha_{z_i} N(x_i | \mu_{z_i},\Sigma_{z_i})}{\sum_{l=1}^k \alpha_l N(x_i | \mu_l,\Sigma_l)}$$

- **【求解$\Theta^{(g+1)}$：M-step】**
将上述函数的定义代入$Q(\Theta, \Theta^{(g)})$，求导令其为零，得到$\Theta^{(g+1)}$的值：$$\alpha_l^{(g+1)} = \frac{\sum_{i=1}^n p(l | x_i,\Theta^{(g)})}{n}$$
$$\mu_l^{(g+1)} = \frac{\sum_{i=1}^n x_i p(l | x_i,\Theta^{(g)})}{\sum_{i=1}^n p(l | x_i,\Theta^{(g)})}$$
$$\Sigma_l^{(g+1)} = \frac{\sum_{i=1}^n [x_i - \mu_l^{(i+1)}] [x_i - \mu_l^{(i+1)}]^T p(l | x_i,\Theta^{(g)})}{\sum_{i=1}^n p(l | x_i,\Theta^{(g)})}$$
其中，$p(l | x_i,\Theta^{(g)})$称为`responsibility  probability`，它是指当得到样本点$x_i$后，该样本点属于第$l$个高斯分布的概率。
